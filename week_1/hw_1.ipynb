{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Cognitive Neuroscience - Homework 1 (Neural Networks)\n",
    "**Start date: 21st January 2021**\n",
    "\n",
    "**Due date (UPDATED!): 1st February 2021**\n",
    "\n",
    "This homework set focuses and expands upon chapters 3, 4 and 7 of the Rojas [book](http://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf). Task is to implement simple perceptrons and neural networks in pure python, from scratch.\n",
    "\n",
    "## Submission instructions\n",
    "Submission is by email to hermanni.halva@helsinki.fi. Follow these instructions to submit:  \n",
    "1. Title of the email: \"ccn homework 1 - student_number\"\n",
    "2. When you have completed the exercises, save the notebook. Attach it to the email.\n",
    "3. Also download a pdf of the notebook and attach it.\n",
    "\n",
    "## IMPORTANT\n",
    "1. Don't share your code and answers with others.\n",
    "2. It's your responsibility to ensure that the notebook has fully finished running all the cells, all the plots view properly etc. before submitting it. I will not re-run any code.\n",
    "3. Submit your work by the deadline.\n",
    "4. If you are confused, think there is a mistake or find things too difficult, just ask on github and I'll help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up -- do not change\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - simple perceptron learning algorithm (15 pts)\n",
    "\n",
    "In this question you are given data, shown below, which comes from two different sources. Your task is to train a simple perceptron (ch. 3, Rojas) using the perceptron learning algorithm (ch. 4, Rojas).\n",
    "\n",
    "More specifically, in the forward pass of the model we calculate, for a single observation $x_i \\in \\mathbb{R}^d$: \n",
    "$$ \\text{score}_i = x_i^Tw +b,$$\n",
    "where $w$ is vector of edge weights and $b$ is a scalar bias. The score determines whether an observation is classified as $1$ or $0$:\n",
    "$$\\hat{y}_i=1 \\quad \\text{if} \\quad \\text{score}_i > 0$$\n",
    "$$\\hat{y}_i=0 \\quad \\text{if} \\quad \\text{score}_i \\le 0.$$\n",
    "\n",
    "Model training is, in this implementation, done by the perceptron learning algorithm described on page 85 of Rojas. Training progress is controlled by a simple loss function $$\\text{loss} = \\sum |y_i-\\hat{y}_i|.$$ where $y_i$ is the true label and $\\hat{y}_i$ is the predicted label.\n",
    "\n",
    "**Your task is** to fill in the missing lines of the code in the second cell below (first cell is data generation -- do not change it). Once the code is properly implemented, you should be able to reach loss of zero very quickly. Notice that the implementation requires the algorithm in matrices and vectors for the whole data rather than a single data point at time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2D data -- do not change\n",
    "N = 100\n",
    "mu1 = np.array([0.3, 0.3])\n",
    "cov1 = np.array([[0.01, 0.],[0., 0.01]])\n",
    "mu2 = np.array([0.8, 0.8])\n",
    "cov2 = np.array([[0., 0.01],[0.01, 0.]])\n",
    "\n",
    "mvn1 = np.random.multivariate_normal(mu1, cov1, size=N)\n",
    "mvn2 = np.random.multivariate_normal(mu2, cov1, size=N)\n",
    "\n",
    "x = np.concatenate((mvn1, mvn2), 0)\n",
    "y = np.concatenate((np.zeros(N), np.ones(N)))\n",
    "\n",
    "# plot the data -- do not change\n",
    "sns.scatterplot(x=mvn1[:,0], y=mvn1[:,1], s=10, color=\"Blue\")\n",
    "sns.scatterplot(x=mvn2[:,0], y=mvn2[:,1], s=10, color=\"Green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_loss(predicted_labels, true_labels):\n",
    "    return #!IMPLEMENT\n",
    "\n",
    "# implement the missing parts\n",
    "class simple_perceptron_unit:\n",
    "    def __init__(self, d):\n",
    "        # initialize weights and bias at random values -- no need to change\n",
    "        self.w = np.random.random(size=(d,))\n",
    "        self.b = np.random.random()\n",
    "        \n",
    "    def forward_pass(self, x):\n",
    "        score = #[[CALCULATE THE SCORE]]\n",
    "        return #[[OUTPUT CLASSIFICATION USING THRESHOLD AS IN INSTRUCTIOS]]\n",
    "    \n",
    "    def train(self, x, y):\n",
    "        # X is input, y the labels\n",
    "        # [[IMPLEMENT THE PERCEPTRON TRAINING ALGORITHM]]\n",
    "            \n",
    "# train the model -- don't change\n",
    "spu = simple_perceptron_unit(2)\n",
    "spu.train(x, y)\n",
    "print(spu.w, spu.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2  - modular view of neural networks (20 pts)\n",
    "\n",
    "As discussed in ch.7, it is best to view the neural networks as sequential graph of modules/functions. Each module takes inputs (data+parameters) and gives outputs. This allows learning to happen through the backpropagation algorithm, as gradients for the entire network can easily be calculated using chain rule. In particular, all we have to do is for each module separately calculate the gradients of its outputs with respect to its input data and parameters (if there are any). Once we know these, we can build a model with modules in any order we desire and easily backpropagate through the chain of modules. **Your task** is to implement the following neural network building blocks by defining their:\n",
    "1. forward pass (input->output)\n",
    "2. backward pass (gradients of module output wrt. module inputs, and wrt. parameters if they exist)\n",
    "\n",
    "**Requirements**\n",
    "- everything should be in matrix/vector form such that it can handle multiple observations of high dimension in parallel\n",
    "- you are allowed to only use native python packages as well as numpy as scipy. No autograd, no pytorch/tensorflow/jax or any other autodiff packages. You need to manually implement tha backpropagation rules.\n",
    "\n",
    "I have given you the first one as an example. **Please see the slides** from thursday for formal definitions of all these modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid_activation:\n",
    "    def __init__(self):\n",
    "        self.X = 0\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        # if you wish to use any results from forward pass later in the backward pass, use self.\n",
    "        # to store it. This is useful for sigmoid, as can be seen here. But it's not always needed. \n",
    "        sigmoid = (1.+np.exp(-X))**-1\n",
    "        self.sigmoid = sigmoid\n",
    "        return sigmoid\n",
    "    \n",
    "    def backprop(self, dL_dy):\n",
    "        # dL_dy = dL/dy is the derivative of the loss function with respect to\n",
    "        # the output of the sigmoid function.\n",
    "        dL_dX = self.sigmoid*(1.-self.sigmoid) * dL_dy\n",
    "        return dL_dX\n",
    "\n",
    "    \n",
    "class relu_activation:\n",
    "  def __init__(self):\n",
    "    self.X = 0\n",
    "  \n",
    "  def forward_pass(self, X):\n",
    "   #[[IMPLEMENT FORWARD PASS]]\n",
    "   return \n",
    "\n",
    "  def backprop(self, dL_dy):\n",
    "    dL_dX = #[[IMPLEMENT]]\n",
    "    return dL_dX\n",
    "\n",
    "    \n",
    "class linear_layer:\n",
    "  def __init__(self, W, b):\n",
    "    self.W = W\n",
    "    self.b = b\n",
    "    self.X = 0\n",
    "    \n",
    "  def forward_pass(self, X):\n",
    "   #[[IMPLEMENT FORWARD PASS]]\n",
    "   return    \n",
    "\n",
    "  def backprop(self, dL_dy):\n",
    "    dL_dW = #[[IMPLEMENT]]\n",
    "    dL_dX = #[[IMPLEMENT]]\n",
    "    dL_db = #[[IMPLEMENT]]\n",
    "    return dL_dW, dL_db, dL_dX\n",
    "  \n",
    "\n",
    "\n",
    "class softmax_crossentropy_loss:\n",
    "  def __init__(self):\n",
    "    self.loss = 0\n",
    "  \n",
    "  def forward_pass(self, scores, true_labels):\n",
    "    #[[IMPLEMENT FORWARD PASS]]\n",
    "    return \n",
    "  \n",
    "  def backprop(self, scores, true_labels):\n",
    "    dL_dscores = #[[IMPLEMENT]]\n",
    "    return dL_dscores\n",
    "\n",
    "\n",
    "# evaluation below -- do not change\n",
    "x = np.random.normal(size=(1000, 100))\n",
    "dL_dy = np.random.normal(size=(1000, 100))\n",
    "W = np.random.normal(size=(100, 50))\n",
    "b = np.random.normal(size=(50, 1))\n",
    "dL_dy_2 = np.random.normal(size=(1000, 50))\n",
    "yhats = np.random.normal(size=(100, 10))\n",
    "ys = np.random.choice([0., 1.], size=(100, 10))\n",
    "\n",
    "\n",
    "sigmoid = sigmoid_activation()\n",
    "print(\"sigmoid fwd pass\", sigmoid.forward_pass(x))\n",
    "print(\"sigmoid backprop\", sigmoid.backprop(dL_dy))\n",
    "\n",
    "relu = relu_activation()\n",
    "print(\"relu fwd pass\", relu.forward_pass(x))\n",
    "print(\"relu backprop\", relu.backprop(dL_dy))\n",
    "\n",
    "linear = linear_layer(W, b)\n",
    "print(\"linear layer fwd pass\", linear.forward_pass(x))\n",
    "print(\"linear layer backprop\", linear.backprop(dL_dy_2))\n",
    "\n",
    "smxe = softmax_crossentropy_loss()\n",
    "print(\"softmaxXE fwd pass\", smxe.forward_pass(yhats, ys))\n",
    "#print(\"softmaxXE backprop\", loss.backprop(yhats, true_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 - building your own neural network (20 pts)\n",
    "\n",
    "In this question you will build your own neural network using a popular deep learning package [PyTorch](https://pytorch.org/). The benefit of this and other similar packages (Tensorflow, JAX etc.) is that they have readily implemented modules (similar to ones you created in previous question) and importantly they use autograd which automatically calculates gradients of those modules for you and thus easily performs backprop. All you need to do is plug these modules together into your desired model. \n",
    "\n",
    "The data you will use is the famous [MNIST](https://en.wikipedia.org/wiki/MNIST_database) image data. The idea is to train the neural network to be able to differentiate between images of the different digits 0-9. \n",
    "\n",
    "**Your task** is to implement a deep neural network that succesfully performs this task. The structure of your neural network should be:\n",
    "\n",
    "input data (batch_size X 784 matrix) $\\rightarrow$ linear layer (9216 units) $\\rightarrow$ relu activation $\\rightarrow$ linear layer (124 units) $\\rightarrow$ output layer (10 unit linear layer)\n",
    "\n",
    "**Important:**\n",
    "   1. you should implement training in minibatches defined by batch_size (in order to do SGD)\n",
    "   2. each minibatch by default is in the shape of (batch_size, 1, 28, 28) since it's a square image of 28x28 pixels with 1 color channel, you need to flatten each observation into a 784 long vector i.e reshape each batch to shape (batch_size, 784) matrix.\n",
    "   3. loss function to use is softmax crossentropy. note that it is implemented outside of the neural network i.e.  not include in the 'class Net()' below\n",
    "   4. You should test accuracy of 98% at least after 5 epochs. Play around with batch_size and learning_rate settings to reach this\n",
    "   5. I have given you the skeleton code so all you need to do is fill in the missing parts, denoted by [[DIRECTIONS]]. If you still find it too difficult, you can just cheat and look here but try to understand how things work: https://github.com/pytorch/examples/blob/master/mnist/main.py of course the network and code there is bit different but should give you more than enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting & data preparation -- do NOT change\n",
    "torch.manual_seed(1)\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "train_data = datasets.MNIST('../data', train=True,\n",
    "                            download=True,\n",
    "                            transform=transform)\n",
    "test_data = datasets.MNIST('../data', train=False,\n",
    "                          transform=transform)\n",
    "log_interval = 100\n",
    "\n",
    "# define the structure of your neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #[[IMPLEMENT: SEE LINK GIVEN IF CONFUSED]]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #[[IMPLEMENT FORWARD PASS OF YOUR NETWORK]]\n",
    "        return\n",
    "\n",
    "# define training loop for a specific epoch\n",
    "def train(model, train_loader, optimizer, current_epoch):\n",
    "    model.train() # needed for training\n",
    "    for batch_idx, (data, true_labels) in enumerate(train_loader): # this loops over minibatches for one epoch\n",
    "        data = #[[RESHAPE DATA AS DESCRIBED ABOVE]]\n",
    "        optimizer.zero_grad() # this clears gradients from previous run\n",
    "        scores = #[[IMPLEMENT TO CALCULATE OUTPUT LAYER SOCRES]]\n",
    "        loss = #[[IMPLEMENT LOSS FOR THE MINIBATCH]]\n",
    "        loss.backward() # this performs the backprop automatically\n",
    "        optimizer.step() # this performs the gradient descent step\n",
    "        if batch_idx % log_interval == 0: # print results\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    " \n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval() # needed for training\n",
    "    test_loss = 0 # initialize loss\n",
    "    correct_predictions = 0 # variable to count correct predictions\n",
    "    with torch.no_grad(): # since no gradients needed in testing the model\n",
    "        for data, true_labels in test_loader: # testing loop readily defined\n",
    "            data = #[[RESHAPE YOUR DATA HERE]]\n",
    "            scores = #[[CALCULATE SCORES FOR THE BATCH]]\n",
    "            test_loss += #[[CALCULATE TOTAL LOSS FOR THE BATCH (SUM ACROSS BATCH)]]\n",
    "            predicted_labels = #[[CHOOSE MOST LIKELY LABEL AS THE PREDICTION FOR EACH OBSERVATION]] \n",
    "            correct_predictions += #[[CALCULATE TOTAL NUMBER OF CORRECT PREDICTIONS FOR THE BATCH]]\n",
    "\n",
    "    test_loss /= len(test_loader.dataset) # calculates average log loss\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct_predictions, len(test_loader.dataset),\n",
    "        100. * correct_predictions / len(test_loader.dataset)))\n",
    "    \n",
    "# set values to desired\n",
    "train_batch_size = #[[CHOOSE MINIBATCH SIZE]]\n",
    "test_batch_size = 1000\n",
    "epochs = #[[SET NUMBER OF TRAINING EPOCHS]]\n",
    "learning_rate = #[[SET LEARNING RATE]]\n",
    "\n",
    "# set data-loaders -- do not change\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=train_batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=test_batch_size)\n",
    "\n",
    "# train and evaluate your model\n",
    "model = #[[INITIALIZE YOUR MODEL]]\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9) # do not change\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus question (optional) - softmax cross-entropy loss and its derivatives (15 pts)\n",
    "\n",
    "This is question is not compulsory for full points, but the points you gain here can be used to cover any points you lost in the previous question.\n",
    "\n",
    "\n",
    "Consider the softmax cross-entropy loss function which you implemented in Question 2.\n",
    "Formally, recall its definition from the slides, for a single observation:\n",
    "$$\n",
    "\\mathrm{loss}\n",
    "~~=~~\n",
    "-\\sum_{i=1}^N \\log{ \\underbrace{\\left(\\frac{\\exp(\\mathbf{z}_{i}[y_i])}{\\sum_{c=1}^{10} \\exp(\\mathbf{z}_{i}[c])}\\right)}_{\\text{softmax output}}}\n",
    "~~=~~\n",
    "\\sum_{i=1}^N \\left( -\\mathbf{z}_{i}[y_i] + \\log{\\left( \\sum_{c=1}^{10} \\exp(\\mathbf{z}_{i}[c]) \\right)} \\right)$$\n",
    "where $\\mathbf{z}_i \\in \\mathbb{R}^{10}$ is the input to the softmax layer for observation $i$ and the notation $\\mathbf{z}_i{[c]}$ denotes the $c$-th entry of vector $\\mathbf{z}_i$. The dataset is $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N$, where $y_i$ is the correct label for that observation.\n",
    "\n",
    "**Your task** is to do the following derivations (no coding needed in this question) and write them below (latex works in the notebook). \n",
    "\n",
    "\n",
    "1. Given the cross-entropy loss above for a single observation, compute the derivative of the loss function with respect to entire minibatch input matrix $\\mathbf{Z}$ i.e. a matrix with shape (batch_size, 10). Any answer should be fully vectorized. [5 pts] \n",
    "$$\\frac{\\partial loss}{\\partial \\mathbf{Z}} = ?$$\n",
    "\n",
    "2. Consider a simple model (input data X --> linear layer --> softmax cross-entropy). Compute the derivatives below, in a fully vectorized and general form with respect to\n",
    "  * the input $\\mathbf{X}$\n",
    "  $$\\frac{\\partial loss}{\\partial \\mathbf{X}} = ?$$\n",
    "  * the parameters of the linear layer: weights $\\mathbf{W}$ and bias $\\mathbf{b}$\n",
    "  $$\\frac{\\partial loss}{\\partial \\mathbf{W}} = ?$$\n",
    "  $$\\frac{\\partial loss}{\\partial \\mathbf{b}} = ?$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
