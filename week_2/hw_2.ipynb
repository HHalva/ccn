{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Cognitive Neuroscience - Homework 2\n",
    "**Start date: 1st February 2021**\n",
    "\n",
    "**Due date: 8th February 2021**\n",
    "\n",
    "This homework set focuses and expands upon the chapters 12 and 13 of the Rojas [book](http://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf) with focus on autoassociative memories, hebbian learning and hopfield networks.\n",
    "\n",
    "## Submission instructions\n",
    "Submission is by email to hermanni.halva@helsinki.fi. Follow these instructions to submit:  \n",
    "1. Title of the email: \"ccn homework 2 - student_number\"\n",
    "2. When you have completed the exercises, save the notebook. Attach it to the email.\n",
    "3. Also download a pdf of the notebook and attach it.\n",
    "\n",
    "## IMPORTANT\n",
    "1. Don't share your code and answers with others.\n",
    "2. It's your responsibility to ensure that the notebook has fully finished running all the cells, all the plots view properly etc. before submitting it. I will not re-run any code.\n",
    "3. Submit your work by the deadline.\n",
    "4. If you are confused, think there is a mistake or find things too difficult, just ask on github\n",
    "5. If you need help with code, email it to me and I'll have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up -- do not change\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 [15 pts]\n",
    "\n",
    "You are given 10 samples from the MNIST data set you encountered last week i.e. one observation for each of the digits 0 to 9. Each of these is flattened into 784 long vector. Thus the data set we have is a matrix $\\mathbf{X}$ of size $10\\times 784$, with each row containing a digit. **Your task is to:**\n",
    "1. Implement an associative hebbian learning network that maps from digit data X_train to their labels Y_train [15 pts]\n",
    "2. Test the weight matrix you learned on the training data, does it work? Explain your observation (either by commenting code or in a seperate text cell). Create your own data for which the algorithm works perfectly -- illustrate that this is the caseÂ [5 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data loading -- do not change\n",
    "data = datasets.MNIST('../data', train=False,\n",
    "                      transform=transform)\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=1)\n",
    "digits = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "X_train = np.zeros((10, 784))\n",
    "Y_train = np.ones((10, 10))*-1\n",
    "\n",
    "for i in digits:\n",
    "    for x, y in data_loader:\n",
    "        if y.item() == i:\n",
    "            x_tr = x.reshape((x.shape[0], -1))\n",
    "            X_train[i,:] = np.where(x_tr < 0, -1, 1)\n",
    "            Y_train[i, i] = 1\n",
    "        else:\n",
    "            next\n",
    "    \n",
    "# you can use below to see what the training images look like if curious\n",
    "example_digit = 0\n",
    "plt.imshow(X_train[example_digit].reshape(28,28), cmap=\"gray\")\n",
    "\n",
    "# START HERE:\n",
    "def hebbian_learning(train_data, train_labels):\n",
    "    #! [[IMPLEMENT]]\n",
    "    return \n",
    "\n",
    "# train\n",
    "W = hebbian_learning(X_train, Y_train)\n",
    "Y_predicted = #! [[IMPLEMENT]]\n",
    "\n",
    "#! [[FOLLOW INSTRUCTIONS IN THE QUESTION FOR THE REST]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 [10 pts]\n",
    "\n",
    "Consider a bidirectional associative memory with\n",
    "\n",
    "$$\\mathbf{W} = \\begin{bmatrix} -1 & 2 \\\\ 1 & 3 \\end{bmatrix},\\, \\mathbf{x}=[1, -1] ,\\, \\mathbf{y}=[1, 1].$$\n",
    "\n",
    "**Your tasks** is to\n",
    "1. Calculate the BAM's energy (analytical/computational solutions both OK) [3 pts]\n",
    "2. Show analytically (no programming) whether this is a local minimum or not. justify this by finding the local minimum (write answer in latex below) [7 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 [20 pts]\n",
    "\n",
    " **Your task:**\n",
    "1. Implement a bidirectional associatie memory algoirthm (Rojas 13.1). You are expected to write it from scratch so no code skeleton is given here though you can use any code from the above question. Show that with an input X it reaches a stable state. [10 pts]\n",
    "2. Same as above but now for Hopfield network [10 pts]\n",
    "\n",
    "You have fair amount of freedom in how you write the code so **please explain (by commenting the code)** as to what different parts of the code do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4 [10 pts]\n",
    "\n",
    "Consider [Conway's Game of Life](https://en.wikipedia.org/wiki/Conway's_Game_of_Life). Show that it can **not** be simulated by a Hopfield network. No programming needed, write your answer below."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
