{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Cognitive Neuroscience - Homework 4\n",
    "**Start date: 15th February 2021**\n",
    "\n",
    "**Due date: 22nd February 2021**\n",
    "\n",
    "This homework set focuses and expands upon the chapters 5, 6 and 7 of the NIS [book](https://www.cs.helsinki.fi/u/ahyvarin/natimgsx/nis_preprintFeb2009.pdf).\n",
    "\n",
    "## Submission instructions\n",
    "Submission is by email to hermanni.halva@helsinki.fi. Follow these instructions to submit:  \n",
    "1. Title of the email: \"ccn homework 4 - student_number\"\n",
    "2. When you have completed the exercises, save the notebook. Attach it to the email.\n",
    "3. Also download a pdf of the notebook and attach it.\n",
    "\n",
    "## IMPORTANT\n",
    "1. Don't share your code and answers with others.\n",
    "2. It's your responsibility to ensure that the notebook has fully finished running all the cells, all the plots view properly etc. before submitting it. I will not re-run any code.\n",
    "3. Submit your work by the deadline.\n",
    "4. If you are confused, think there is a mistake or find things too difficult, just ask on github\n",
    "5. If you need help with code, email it to me and I'll have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up -- do not change\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data -- do not change (unless need to change patch size; see below)\n",
    "num_patches = 1000\n",
    "patch_size = 64\n",
    "all_I = np.zeros((num_patches, patch_size**2))\n",
    "\n",
    "for i in range(num_patches):\n",
    "    img_idx = np.random.choice(range(13))  \n",
    "    im = Image.open(str(img_idx+1) +'.tiff')\n",
    "    max_top = np.array(im).shape[0]-patch_size\n",
    "    max_left = np.array(im).shape[1]-patch_size\n",
    "    top = np.random.choice(range(max_top+1))\n",
    "    left = np.random.choice(range(max_left+1))\n",
    "    bottom = top+patch_size\n",
    "    right = left+patch_size\n",
    "    im = im.crop((left, top, right, bottom))\n",
    "    # plot first ten examples\n",
    "    if i < 10:\n",
    "        plt.figure(figsize=(2, 2))\n",
    "        plt.imshow(im)\n",
    "        plt.set_cmap('gray')\n",
    "        plt.show()\n",
    "    all_I[i] = np.array(im, dtype=np.double).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 - PCA on image patches [25 pts]\n",
    "In order to do PCA, we need data. This data is in the compressed file in the github folder. Extract it into your working directory. If done correctly, the cell above can be used to load the data. Specifically, it creates variable 'all_I' which contains 1000 flattened 64x64 image patches. If this is too much for your computer to handle, you can use 32x32 by modifying the above code. Use that data to complete this exercise, which comes in two parts.\n",
    "\n",
    "**Part a.)**\n",
    "\n",
    "Write a function which performs all the steps of the canonical preprocessing as described in section 5.4. Everything has to be written in pure numpy / base python. PCA should be computed via eigenvalue decomposition. To illustrate that you have implemented PCA correctly, plot the weights of the 10 first principal components.\n",
    "\n",
    "**Part b.)**\n",
    "\n",
    "Answer the following questions:\n",
    "1. For the 10 first PCs, compute and plot the percentage of total variance each of them explains. What does this say about the usefulness of the \"higher\" components? Does your conclusion make sense from theoretical point of view (justify)?\n",
    "2. Show analytically that, after DC removal, the mean output of a simple feature detector is zero.\n",
    "3. Are the principal components you obtain (pre-whitening) orthogonal? Show this analytically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_canonical(x):\n",
    "    return  #! [[TO IMPLEMENT]]\n",
    "\n",
    "\n",
    "# perform pre-processing on data\n",
    "\n",
    "# plot pca weights \n",
    "    \n",
    "# plot R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 - sparse coding [10 pts]\n",
    "Answer following questions (numpy/base python only **no scipy**):\n",
    "1. Sample 1000 data points from each the Laplace distribution and the uniform distribution (with zero mean). Standardize them to unit variance and computer their kurtoses.\n",
    "2. Sample 1000 data points from two independent Laplacian distributions, standardize both to unit variance. For each pair of draws, compute the sum of the two variables and standardize this as well. Compute kurtosis of the standardized sum.\n",
    "3. Repeat 2. with uniform distribution.\n",
    "4. How do the above relate to the central limit theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "\n",
    "# kurtosis function\n",
    "def kurtosis(x):\n",
    "    #! [[IMPLEMENT]] \n",
    "    return \n",
    "\n",
    "draws=1000\n",
    "\n",
    "# Question 1\n",
    "\n",
    "\n",
    "# Question 2\n",
    "\n",
    "\n",
    "# Question 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 - ICA [20 pts]\n",
    "1. Prove equation (7.2)\n",
    "2. Based on (7.2), prove that two independent random variables are uncorrelated\n",
    "3. ICA likelihood can be written in the form $$p(\\mathbf{x})=p_s(\\mathbf{A}^{-1}\\mathbf{x})/|\\det \\mathbf{A}|,$$ where $\\mathbf{x}$ is an observed data vector and $p_s(\\cdot)$ the distribution function of the independenct components. Consider taking derivatives with respect to $\\mathbf{A}$, show that one can reach a following learning rule: $$\\Delta {\\mathbf{A}} \\propto \\langle \\left[\\mathbf{A}^T\\right]^{-1} \\mathbf{z(s)}\\mathbf{s}^T - \\left[\\mathbf{A}^T\\right]^{-1} \\rangle,$$ with appropriately defined $\\mathbf{z(s)}$. $\\langle\\cdot\\rangle$ denote empirical average.\n",
    "4. Play around with some ready-made ICA implementation such as [fastICA](https://scikit-learn.org/stable/modules/generated/fastica-function.html?highlight=ica) and see if you can create a plot similar to Fig. 7.3. in the book. You can use the data, and code, from Q1, or anything else appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import fastica"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
