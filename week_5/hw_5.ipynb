{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Cognitive Neuroscience - Homework 5\n",
    "**Start date: 22nd February 2021**\n",
    "\n",
    "**Due date: 1st March 2021**\n",
    "\n",
    "This homework set focuses and expands upon the chapters 9, 10 and 11 of the NIS [book](https://www.cs.helsinki.fi/u/ahyvarin/natimgsx/nis_preprintFeb2009.pdf).\n",
    "\n",
    "## Submission instructions\n",
    "Submission is by email to hermanni.halva@helsinki.fi. Follow these instructions to submit:  \n",
    "1. Title of the email: \"ccn homework 5 - student_number\"\n",
    "2. When you have completed the exercises, save the notebook. Attach it to the email.\n",
    "3. Also download a pdf of the notebook and attach it.\n",
    "\n",
    "## IMPORTANT\n",
    "1. Don't share your code and answers with others.\n",
    "2. It's your responsibility to ensure that the notebook has fully finished running all the cells, all the plots view properly etc. before submitting it. I will not re-run any code.\n",
    "3. Submit your work by the deadline.\n",
    "4. If you are confused, think there is a mistake or find things too difficult, just ask on github\n",
    "5. If you need help with code, email it to me and I'll have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up -- do not change\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create data -- do not change (unless need to change patch size)\n",
    "num_patches = 1000\n",
    "patch_size = 128\n",
    "all_I = np.zeros((num_patches, patch_size**2))\n",
    "\n",
    "for i in range(num_patches):\n",
    "    img_idx = np.random.choice(range(13))  \n",
    "    im = Image.open(str(img_idx+1) +'.tiff')\n",
    "    max_top = np.array(im).shape[0]-patch_size\n",
    "    max_left = np.array(im).shape[1]-patch_size\n",
    "    top = np.random.choice(range(max_top+1))\n",
    "    left = np.random.choice(range(max_left+1))\n",
    "    bottom = top+patch_size\n",
    "    right = left+patch_size\n",
    "    im = im.crop((left, top, right, bottom))\n",
    "    # plot first ten examples\n",
    "    if i < 10:\n",
    "        plt.figure(figsize=(2, 2))\n",
    "        plt.imshow(im)\n",
    "        plt.set_cmap('gray')\n",
    "        plt.show()\n",
    "    all_I[i] = np.array(im, dtype=np.double).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1  - ICA with contrast gain control [15 pts]\n",
    "\n",
    "This question explores energy correlations and contrast gain control as discussed in chapter 9. We use the same data set as last week, loaded above, and the (fast)ICA implementation from the scikit learn package\n",
    "**Your task** is to:\n",
    "1. Compute ICA and its output components on the image patches and calculate whether the resulting components are fully independent. Show results with an appropriate plot or similar.\n",
    "2. Implement code for contrast gain control (9.13) and show whether it fixes any issues. Why / why not?\n",
    "3. If PCA already accounts for linear correlations, and yet linear ICA doesn't get rid of nonlinear dependencies between variables, what does ICA then exactly account for? Qualitative answer is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "#[[!IMPLEMENTATIONS HERE]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 - energy detectors [20 pts]\n",
    "Answer the following mathemical exercises from the book -- no coding here.\n",
    "\n",
    "**1.** Show equation (10.6)\n",
    "\n",
    "**2.** This exercise considers the simplest case of steerable filters. Consider the gaussian function\n",
    "$$ \\phi(x, y) = \\exp(-\\frac{1}{2}(x^2 + y^2))$$\n",
    "**a.)** compute its partial derivatives with respect to x and y, denoted by $\\phi_x$ and $\\phi_y$ \n",
    "\n",
    "**b.)** show that the two partial derivative functions are orthogonal\n",
    "   \n",
    "**c.)** the two functions $\\phi_x$ and $\\phi_y$ define steerable filters. The subspace they span has an invarance property:  Define an orientation angle parameter $\\alpha$  and consider linear combination\n",
    "    $$\\phi_{\\alpha} = \\phi_x \\cos \\alpha + \\phi_y\\sin \\alpha.$$\n",
    "    The point is to show that $\\phi_{\\alpha}$ has the same shape as $\\phi_x$ or $\\phi_y$, only difference being that they are rotated versions of each other. Thus, $\\phi_x$, and $\\phi_y$, form an orthogonal basis for a subspace which consists of simple edge detectors with all possible orientations. The proof can be obtained as follows. Define rotated version of the variables as \n",
    "    $$\\begin{pmatrix}x' \\\\ y' \\end{pmatrix}=\\begin{pmatrix} \\sin\\beta & \\cos \\beta \\\\ -\\cos\\beta & \\sin\\beta \\end{pmatrix}\\begin{pmatrix}x \\\\ y \\end{pmatrix}$$\n",
    "    express $\\phi$ as a function of $x'$ and $y'$. Show that this is equivalent to $\\phi_{\\alpha}$ for a suitably chosen $\\beta$.\n",
    "\n",
    "**3.** The likelihood for a subspace in the ISA model is given as $p(s_1, s_2)=\\frac{2}{3\\pi}\\exp(-\\sqrt{3}\\sqrt{s_1^2+s_2^2})$. Use the substituions $s_1=r\\cos\\theta$, $s_2=r\\sin\\theta$ to compute this probability distribution in terms of $r$ and $\\theta$. Show that the final distribution is independent across these variables. Make sure that your distribution is properly normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 - topographic ICA [20 pts]\n",
    "1. Synthesize images from topographic ICA i.e. attempt to replicate Fig. 11.9. Follow the ideas in section 11.7.1.3. Note that you do **not** have to implement  the full generative TICA model.\n",
    "2. Consider the likelihood term in equation (11.5). Assume that $h(x)=-\\alpha \\sqrt{x}+\\beta$. Derive, analytically, the gradient of the likelihood with respect to the weight vectors. Write your your result in matrix/vector form (where possible) and simplify as far as possible.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
