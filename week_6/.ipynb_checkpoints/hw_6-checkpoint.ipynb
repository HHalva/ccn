{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Cognitive Neuroscience - Homework 6\n",
    "**Start date: 1st March 2021**\n",
    "\n",
    "**Due date: 8th March 2021**\n",
    "\n",
    "This homework set focuses and expands upon the chapters 12, 13 and 14 of the NIS [book](https://www.cs.helsinki.fi/u/ahyvarin/natimgsx/nis_preprintFeb2009.pdf).\n",
    "\n",
    "## Submission instructions\n",
    "Submission is by email to hermanni.halva@helsinki.fi. Follow these instructions to submit:  \n",
    "1. Title of the email: \"ccn homework 6 - student_number\"\n",
    "2. When you have completed the exercises, save the notebook. Attach it to the email.\n",
    "3. Also download a pdf of the notebook and attach it.\n",
    "\n",
    "## IMPORTANT\n",
    "1. Don't share your code and answers with others.\n",
    "2. It's your responsibility to ensure that the notebook has fully finished running all the cells, all the plots view properly etc. before submitting it. I will not re-run any code.\n",
    "3. Submit your work by the deadline.\n",
    "4. If you are confused, think there is a mistake or find things too difficult, just ask on github\n",
    "5. If you need help with code, email it to me and I'll have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up -- do not change\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data -- do not change (unless need to change patch size)\n",
    "num_patches = 1000\n",
    "patch_size = 6\n",
    "all_I = np.zeros((num_patches, patch_size**2))\n",
    "\n",
    "for i in range(num_patches):\n",
    "    img_idx = np.random.choice(range(13))  \n",
    "    im = Image.open(str(img_idx+1) +'.tiff')\n",
    "    max_top = np.array(im).shape[0]-patch_size\n",
    "    max_left = np.array(im).shape[1]-patch_size\n",
    "    top = np.random.choice(range(max_top+1))\n",
    "    left = np.random.choice(range(max_left+1))\n",
    "    bottom = top+patch_size\n",
    "    right = left+patch_size\n",
    "    im = im.crop((left, top, right, bottom))\n",
    "    # plot first ten examples\n",
    "    if i < 10:\n",
    "        plt.figure(figsize=(2, 2))\n",
    "        plt.imshow(im)\n",
    "        plt.set_cmap('gray')\n",
    "        plt.show()\n",
    "    all_I[i] = np.array(im, dtype=np.double).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1  -  V1 two layer model [25 pts]\n",
    "\n",
    "**Part a) [15 pts]**\n",
    "\n",
    "Create a function that implements complex cell responses as per equation (12.1) and does so for 432 different configurations as explained on page 275 i.e. all filters are 6x6, there is one centered at each pixel location (36), with 4 different possible orientations, and 3 different frequencies, thus $3\\times4\\times36$ combinations. You can choose the parameters yourself, but make them reasonable. Illustrate your code by calculating this 432 long output on a series of image patches (use above code and data from previous weeks to load patches). Here you can assume that each image patch is the same size as your gabor filter so that we dont need to worry about convolving over the patch; to reiterate, each image patch should give out a 432 long vector.\n",
    "\n",
    "Some guidance -- In order to do this, you probably need to :\n",
    "1. create gabor filter weights in odd-even quadrature phase pairing (feel free to use your code from week 3, but you may likely need to modify so that can easily calculate for all the different combinations)\n",
    "2. implement (12.1) in a sensible way so that it's easily computed for all the different parameter combinations\n",
    "3. I have given some code skeleton below, should it help you, buy you are free to disregard it\n",
    "\n",
    "**Part b) [10 pts]**\n",
    "\n",
    "Answer the following questions:\n",
    "1. Explain in a few sentences what is the general significance of complex cells in the V1\n",
    "2. In ch.12 we saw that the complex cell output is followed up by ICA. What is the intuition behind this two layer model and what benefit does it provide over just a simple cell model?\n",
    "3. What is the intuitive interpretation of the type of higher order features that this model outputs; why do you think they are useful to the visual cortex?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code skeleton -- feel free to use or to ignore\n",
    "\n",
    "class GaborFilter:\n",
    "    def __init__(self, dim, loc_x, loc_y, theta, sigma_x, sigma_y, freq_x, phase):\n",
    "        self.d = dim\n",
    "        self.locx = loc_x # could use to change the location of filter\n",
    "        self.locy = loc_y # could use to change the location of filter\n",
    "        self.th = theta # angle for changing orientation of filter\n",
    "        self.sx = sigma_x\n",
    "        self.sy = sigma_y\n",
    "        self.w = freq_x\n",
    "        self.p = phase\n",
    "   \n",
    "    def _gabor_function_2d(self, x, y):\n",
    "        # evaluates 2d gabor function on a specific point\n",
    "        num = # basically what you did week 3\n",
    "        norm =  \n",
    "        return num/norm\n",
    "    \n",
    "    def get_gabor_filter(self):\n",
    "        # creates filter by computing the gabor function over desired size grid\n",
    "        x = np.arange(0, self.d, 1)\n",
    "        y = np.arange(0, self.d, 1)\n",
    "        xx, yy = np.meshgrid(x, y, sparse=True)\n",
    "        x_theta = # rotate to change orientation (recall chapter 3)\n",
    "        y_theta = # rotate to change orientation (recall chapter 3)\n",
    "        return # return desired output\n",
    "        \n",
    "    \n",
    "class ComplexCell:\n",
    "    def __init__(self, dim, loc_x, loc_y,\n",
    "                 theta, sigma_x, sigma_y, freq_x, phase=0):\n",
    "        self.gabor_eve = GaborFilter(#)\n",
    "        self.gabor_odd = GaborFilter(# + #something here?#)\n",
    "        self.We = # ... create the even filter\n",
    "        self.Wo = # ... and odd\n",
    "        \n",
    "    def calc_energy(self, I):\n",
    "        return #...\n",
    "            \n",
    "            \n",
    "#  [[! IMPLEMENT REST BELOW]]\n",
    "# i.e. calculate outputs of all the complex cells for some image patches\n",
    "# this may be useful: for p in itertools.product(*params):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2  -  Overcomplete basis and Bayesian perspective [30 pts]\n",
    "\n",
    "**Part a.) [20 pts]**\n",
    "\n",
    "Consider the Bayes' rule $$p(\\mathbf{s}|\\mathbf{x})=\\frac{p(\\mathbf{x}|\\mathbf{s})p(\\mathbf{s})}{p(\\mathbf{x})}$$\n",
    "\n",
    "1. In MAP estimation we maximize $\\log p(\\mathbf{s}|\\mathbf{x})$ indirectly by maximizing $\\log p(\\mathbf{x}|\\mathbf{s})+\\log p(\\mathbf{s})$. What is the benefit of this instead of just directly specifying the distribution of $\\log p(\\mathbf{s}|\\mathbf{x})$ and maximizing that.\n",
    "\n",
    "2. In the overcomplete model we add noise to the typical ICA model. In fact, we have\n",
    " $$\\mathbf{x} = \\mathbf{A}\\mathbf{s}+\\mathbf{n},$$ where $\\mathbf{x}$ is $d$-dimensional random variable, $\\mathbf{A}$ is $d \\times m$ matrix, $\\mathbf{s}$ is $m$-dimensional random variable, $m >> d$, and $\\mathbf{n}$ is $d$-dimensional noise vector with distribution $p_{\\mathbf{n}}(\\cdot)$. What is the distribution of $p(\\mathbf{x}|\\mathbf{s})$ in terms of $p_{\\mathbf{n}}(\\cdot)$.\n",
    " \n",
    "3. Why do you think noise is added in the overcomplete case? Hint: what would happen to the probability distributions the make up the Bayes rule if there was no noise.\n",
    "\n",
    "4. Assume following (vector)-random variables and their distributions: $p(\\mathbf{s}) = \\mathcal{N}(0, I)$, $p(\\mathbf{x}|\\mathbf{s}) \\sim \\mathcal{N}(\\mathbf{As}, \\phi I)$ for some matrix $\\mathbf{A}$ and scalar $\\phi$. . Derive $p(\\mathbf{s}|\\mathbf{x})$ (show your workings).\n",
    "\n",
    "5. Same set-up as 4. but instead derive $p(\\mathbf{x})$. Hint: law of iterated expectations.\n",
    "\n",
    "**Part b.) [10 pts]**\n",
    "\n",
    "Consider you have a generative model similar to the one discussed above and in section 13.1.3 of the book. Specifically, for a single observation, you have $\\log p(\\mathbf{s}_i|\\mathbf{x}_i)=\\log p(\\mathbf{x}_i|\\mathbf{s}_i)+\\log p(\\mathbf{s}_i)$ where again $p(\\mathbf{x}_i|\\mathbf{s}_i) \\sim \\mathcal{N}(\\mathbf{As}_i, \\phi I)$. Here $\\mathbf{x}_i$ is $2$-dimensional random variable, $\\mathbf{s}_i$ is $10$-dimensional random variable, and thus $\\mathbf{A}$ is $2 \\times 10$ matrix. $\\mathbf{A}, \\phi$ are given in the code below. Each of the 10 elements of $p(\\mathbf{s}_i)$ follow, independently, laplace distribution as defined [here](https://numpy.org/devdocs/reference/random/generated/numpy.random.laplace.html) with the location and scale parameters varying between the elements. \n",
    "\n",
    "1. Generate 10000 samples, i.e. $(\\mathbf{x}_i,\\mathbf{s}_i)$ pairs, from this generative model, without using any loops in your code, preferably.\n",
    "3. Note that the samples are assumed i.i.d, and with this in mind, create a function that computes $\\log p(\\mathbf{s}|\\mathbf{x})$ for the whole data. You are expected to write this from scratch in numpy. but you can always check that your function works by comparing it to e.g. implementations in SciPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change -- these are given\n",
    "import matplotlib.pyplot as plt\n",
    "A = np.random.normal(size=(2, 10))\n",
    "phi = 1.47\n",
    "laplace_locs = np.linspace(1, 10, 10)\n",
    "laplace_scales = np.linspace(1, 3, 10)\n",
    "np.random.shuffle(laplace_scales)\n",
    "\n",
    "# ![[ start by drawing from laplace distribution]]\n",
    "S = np.random.laplace(laplace_locs, laplace_scales, (?, ?))\n",
    "\n",
    "# [[IMPLEMENT REST]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
